import os
import base64
import httpx
import uvicorn
import hashlib
import asyncio
import zipfile
import io
from fastapi import FastAPI, HTTPException, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Optional
from dotenv import load_dotenv
from scraper import XHSScraper

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# === æ ¸å¿ƒé…ç½®åŒº ===
GOOGLE_API_KEY = os.getenv("GEMINI_API_KEY")

# 1. ä¿®æ­£æ¨¡å‹åç§° (åƒä¸‡åˆ«å†™ 2.5)
MODEL_NAME = "gemini-2.5-flash"

# 2. ä½ çš„ Vercel ä»£ç†åœ°å€ (å¦‚æœæœ¬åœ°ç›´è¿ Google è¿˜æ˜¯ä¸è¡Œï¼Œå°±èµ°è¿™ä¸ª)
PROXY_BASE_URL = "https://gemini.sumukeio.xyz"

# 3. ã€å…³é”®ã€‘æœ¬åœ° VPN ä»£ç†åœ°å€
# è¯·æ£€æŸ¥ä½ çš„æ¢¯å­è½¯ä»¶ï¼Œçœ‹"ç«¯å£"æ˜¯å¤šå°‘ã€‚Clash é»˜è®¤æ˜¯ 7890ï¼Œv2ray å¯èƒ½æ˜¯ 10809
LOCAL_VPN_PROXY = None

# ä¸‹è½½å†…å®¹æ ¹ç›®å½•ï¼ˆç›¸å¯¹ backend ç›®å½•ï¼‰
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DOWNLOAD_ROOT = os.path.join(BASE_DIR, "downloads")
os.makedirs(DOWNLOAD_ROOT, exist_ok=True)

if not GOOGLE_API_KEY:
    print("âš ï¸ è­¦å‘Š: æœªæ£€æµ‹åˆ° GEMINI_API_KEYï¼ŒAI åŠŸèƒ½å°†æ— æ³•ä½¿ç”¨ã€‚")

app = FastAPI()

# æ·»åŠ CORSæ”¯æŒ
from fastapi.middleware.cors import CORSMiddleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”è¯¥é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class GenerateRequest(BaseModel):
    url: str

class GeneratedContent(BaseModel):
    title: str
    content: List[str]
    tags: List[str]
    englishHook: str
    images: List[str]
    ocrText: str = "" 


class DownloadRequest(BaseModel):
    """ä¸“ç”¨äºä¸‹è½½åˆ°æœ¬åœ°ç£ç›˜çš„è¯·æ±‚ä½“"""
    url: str
    # å¯é€‰ï¼šè‡ªå®šä¹‰ä¿å­˜æ ¹ç›®å½•ï¼ˆç»å¯¹è·¯å¾„æˆ–ç›¸å¯¹ backend çš„è·¯å¾„ï¼‰
    base_dir: str | None = None


class DownloadResponse(BaseModel):
    """è¿”å›ä¸‹è½½åçš„åŸºæœ¬ä¿¡æ¯"""
    title: str
    folder: str
    text_file: str
    image_files: List[str]


# === æ–°å¢ï¼šæ‰¹é‡è§£æç›¸å…³æ¨¡å‹ ===
class BatchParseRequest(BaseModel):
    """æ‰¹é‡è§£æè¯·æ±‚"""
    urls: List[str]


class ParsedNote(BaseModel):
    """è§£æåçš„ç¬”è®°æ•°æ®ï¼ˆè¿”å›ç»™å‰ç«¯ï¼‰"""
    id: str  # åŸºäºURLç”Ÿæˆçš„å”¯ä¸€ID
    url: str
    title: str
    content: str
    tags: List[str]
    images: List[str]
    coverImage: str | None = None  # å°é¢å›¾ï¼ˆç¬¬ä¸€å¼ ï¼‰


class BatchParseResponse(BaseModel):
    """æ‰¹é‡è§£æå“åº”"""
    notes: List[ParsedNote]
    failed: List[Dict[str, str]]  # [{"url": "...", "error": "..."}]


# === æ–°å¢ï¼šé€‰æ‹©æ€§ä¸‹è½½ç›¸å…³æ¨¡å‹ ===
class SelectiveDownloadRequest(BaseModel):
    """é€‰æ‹©æ€§ä¸‹è½½è¯·æ±‚"""
    note_data: Dict  # ç¬”è®°æ•°æ®ï¼ˆåŒ…å« title, content, tags, images ç­‰ï¼‰
    selected_image_indices: List[int] | None = None  # é€‰ä¸­çš„å›¾ç‰‡ç´¢å¼•ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
    base_dir: str | None = None


class SelectiveDownloadResponse(BaseModel):
    """é€‰æ‹©æ€§ä¸‹è½½å“åº”"""
    title: str
    folder: str
    text_file: str
    image_files: List[str]


# === æ–°å¢ï¼šæ–‡ä»¶å¤¹æµè§ˆç›¸å…³æ¨¡å‹ ===
class BrowseFolderRequest(BaseModel):
    """æµè§ˆæ–‡ä»¶å¤¹è¯·æ±‚"""
    path: Optional[str] = None  # å¦‚æœä¸ºç©ºï¼Œè¿”å›é»˜è®¤è·¯å¾„


class FolderItem(BaseModel):
    """æ–‡ä»¶å¤¹é¡¹"""
    name: str
    path: str
    is_directory: bool


class BrowseFolderResponse(BaseModel):
    """æµè§ˆæ–‡ä»¶å¤¹å“åº”"""
    current_path: str
    items: List[FolderItem]
    parent_path: Optional[str] = None

async def download_image_as_bytes(url: str):
    """
    ä¸‹è½½å›¾ç‰‡ (å›¾ç‰‡é€šå¸¸ä¸éœ€è¦èµ°ä»£ç†ï¼Œæˆ–è€…èµ°ä»£ç†ä¹Ÿè¡Œ)
    è¿™é‡Œä¸ºäº†ç¨³å¦¥ï¼Œæˆ‘ä»¬è®©å›¾ç‰‡ä¸‹è½½ä¹Ÿå°è¯•èµ°ä¸€ä¸‹ä»£ç†ï¼Œæˆ–è€…ç›´è¿
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.xiaohongshu.com/"
    }
    
    # å›¾ç‰‡é€šå¸¸å›½å†…èƒ½è®¿é—®ï¼Œæ‰€ä»¥è¿™é‡Œ proxy=None (ä¸èµ°ä»£ç†)ï¼Œé€Ÿåº¦æ›´å¿«
    # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œå¯ä»¥æ”¹æˆ proxy=LOCAL_VPN_PROXY
    async with httpx.AsyncClient(headers=headers, follow_redirects=True, verify=False) as client:
        try:
            resp = await client.get(url, timeout=15.0)
            if resp.status_code == 200:
                print(f"   - å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {url[:30]}...")
                return {
                    "mime_type": resp.headers.get("content-type", "image/jpeg"),
                    "data": resp.content
                }
            else:
                print(f"   - å›¾ç‰‡ä¸‹è½½å¤±è´¥ (çŠ¶æ€ç  {resp.status_code}): {url[:30]}...")
        except Exception as e:
            print(f"   - å›¾ç‰‡ä¸‹è½½å‡ºé”™: {e}")
    return None

async def call_gemini_via_proxy(prompt: str, image_parts: list):
    """
    é€šè¿‡ Cloudflareçš„Worker è°ƒç”¨ Gemini
    """
    if not GOOGLE_API_KEY:
        return "æœªé…ç½® API Key"

    # æ„é€  URL
    api_url = f"{PROXY_BASE_URL}/v1beta/models/{MODEL_NAME}:generateContent?key={GOOGLE_API_KEY}"
    
    # æ„é€ è¯·æ±‚ä½“
    contents_parts = [{"text": prompt}]
    for img in image_parts:
        b64_data = base64.b64encode(img['data']).decode('utf-8')
        contents_parts.append({
            "inline_data": {
                "mime_type": img['mime_type'],
                "data": b64_data
            }
        })

    payload = {"contents": [{"parts": contents_parts}]}

    print(f"ğŸ“¡ æ­£åœ¨è¿æ¥ Gemini ({MODEL_NAME})...")
    # Cloudflare ä¸€èˆ¬å›½å†…ç›´è¿æ²¡é—®é¢˜ï¼Œä¸éœ€è¦ proxy å‚æ•°
    # verify=False æ˜¯ä¸ºäº†é˜²æ­¢æŸäº› SSL æ¡æ‰‹æŠ¥é”™ï¼ŒåŠ ä¸Šæ›´ç¨³
    async with httpx.AsyncClient(timeout=60.0, verify=False) as client:
        try:
            resp = await client.post(api_url, json=payload)
            
            if resp.status_code != 200:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {resp.status_code} - {resp.text}")
                return f"AI æŠ¥é”™: {resp.status_code}"
            
            result = resp.json()
            try:
                text = result['candidates'][0]['content']['parts'][0]['text']
                return text
            except (KeyError, IndexError):
                print(f"âŒ è§£æå“åº”å¤±è´¥: {result}")
                return "AI è¿”å›æ ¼å¼å¼‚å¸¸"
                
        except Exception as e:
            print(f"âŒ ç½‘ç»œè¿æ¥å¤±è´¥: {e}")
            return "ç½‘ç»œè¿æ¥å¤±è´¥"


def _sanitize_filename(name: str) -> str:
    """
    å°†æ ‡é¢˜è½¬æ¢ä¸ºé€‚åˆä½œä¸ºæ–‡ä»¶/æ–‡ä»¶å¤¹åçš„å­—ç¬¦ä¸²
    - å»é™¤ Windows ä¸å…è®¸çš„å­—ç¬¦: \ / : * ? " < > |
    - å»æ‰å‰åç©ºæ ¼ï¼Œå¹¶é™åˆ¶é•¿åº¦
    """
    invalid_chars = r'\/:*?"<>|'
    sanitized = "".join(c for c in name if c not in invalid_chars)
    sanitized = sanitized.strip()
    if not sanitized:
        sanitized = "xhs_note"
    # é¿å…è·¯å¾„è¿‡é•¿ï¼Œç®€å•é™åˆ¶åˆ° 60 ä¸ªå­—ç¬¦
    return sanitized[:60]


def _generate_note_id(url: str) -> str:
    """åŸºäºURLç”Ÿæˆå”¯ä¸€ID"""
    return hashlib.md5(url.encode()).hexdigest()[:12]


async def _save_note_to_disk(data: Dict, selected_indices: List[int] | None = None) -> Dict:
    """
    æ ¹æ®çˆ¬è™«è¿”å›çš„æ•°æ®ï¼Œå°†å›¾ç‰‡å’Œæ–‡å­—ä¿å­˜åˆ°æœ¬åœ°
    ç›®å½•ç»“æ„ç¤ºä¾‹:
    backend/
      downloads/
        ç¬”è®°æ ‡é¢˜/
          ç¬”è®°æ ‡é¢˜.txt
          image_1.jpg
          image_2.png
    """
    title = data.get("title") or "xhs_note"
    desc = data.get("content") or ""
    tags = data.get("tags") or []
    origin_url = data.get("origin_url") or ""
    images = data.get("images") or []

    folder_name = _sanitize_filename(title)
    folder_path = os.path.join(DOWNLOAD_ROOT, folder_name)
    os.makedirs(folder_path, exist_ok=True)

    # 1. ä¿å­˜æ–‡å­—åˆ° txt
    text_filename = f"{folder_name}.txt"
    text_path = os.path.join(folder_path, text_filename)
    lines = [
        title,
        "",
        desc,
        "",
    ]
    if tags:
        lines.append("æ ‡ç­¾: " + ", ".join(tags))
    if origin_url:
        lines.append(f"æ¥æºé“¾æ¥: {origin_url}")

    with open(text_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    # 2. ä¸‹è½½å¹¶ä¿å­˜å›¾ç‰‡ï¼ˆæ”¯æŒé€‰æ‹©æ€§ä¸‹è½½ï¼‰
    image_files: List[str] = []
    images_to_download = images
    if selected_indices is not None:
        # åªä¸‹è½½é€‰ä¸­çš„å›¾ç‰‡
        images_to_download = [images[i] for i in selected_indices if 0 <= i < len(images)]
    
    for idx, img_url in enumerate(images_to_download, start=1):
        img_data = await download_image_as_bytes(img_url)
        if not img_data:
            continue
        mime = img_data.get("mime_type", "image/jpeg").lower()
        ext = "jpg"
        if "png" in mime:
            ext = "png"
        elif "webp" in mime:
            ext = "webp"
        elif "gif" in mime:
            ext = "gif"

        img_filename = f"image_{idx}.{ext}"
        img_path = os.path.join(folder_path, img_filename)
        try:
            with open(img_path, "wb") as f:
                f.write(img_data["data"])
            image_files.append(img_filename)
        except Exception as e:
            print(f"   - ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")

    return {
        "title": title,
        "folder": folder_name,
        "text_file": text_filename,
        "image_files": image_files,
    }

@app.post("/api/generate", response_model=GeneratedContent)
async def generate_content(request: GenerateRequest):
    print(f"\nğŸš€ [1/3] å¼€å§‹çˆ¬å–: {request.url}")
    
    scraper = XHSScraper()
    try:
        data = await scraper.scrape_note(request.url)
        if not data:
            raise HTTPException(status_code=400, detail="æŠ“å–å¤±è´¥")
    finally:
        await scraper.close()

    print(f"âœ… [2/3] æŠ“å–å®Œæˆ: {data['title']}")

    extracted_text_from_images = ""
    
    if data['images'] and GOOGLE_API_KEY:
        print(f"ğŸ‘€ [3/3] å‡†å¤‡ AI è¯†åˆ« (å…± {len(data['images'])} å¼ )...")
        
        image_parts = []
        # ä¸ºäº†é€Ÿåº¦å’ŒæˆåŠŸç‡ï¼Œå…ˆåªå‘å‰ 3 å¼ 
        for img_url in data['images'][:3]:
            img_data = await download_image_as_bytes(img_url)
            if img_data:
                image_parts.append(img_data)
        
        if image_parts:
            prompt = "ä½ æ˜¯ä¸€ä¸ª OCR åŠ©æ‰‹ã€‚è¯·æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—ï¼Œé‡ç‚¹æå–å¤§å­—æ ‡é¢˜å’Œé‡‘å¥ã€‚ç›´æ¥è¾“å‡ºæ–‡å­—ï¼Œç”¨æ¢è¡Œåˆ†éš”ã€‚"
            extracted_text_from_images = await call_gemini_via_proxy(prompt, image_parts)
            print("âœ… AI è¯†åˆ«æµç¨‹ç»“æŸ")
        else:
            print("âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼Œè·³è¿‡ AI")
    else:
        print("â­ï¸ è·³è¿‡ AI (æ—  Key æˆ– æ— å›¾)")

    content_lines = [line for line in data['content'].split('\n') if line.strip()]
    
    return {
        "title": data['title'],
        "englishHook": "AI EXTRACTED", 
        "content": content_lines, 
        "tags": data['tags'],
        "images": data['images'],
        "ocrText": extracted_text_from_images
    }


@app.post("/api/download_note", response_model=DownloadResponse)
async def download_note(request: DownloadRequest):
    """
    ç‹¬ç«‹çš„â€œçˆ¬å–å¹¶è½ç›˜â€æ¥å£ï¼š
    - ä½¿ç”¨ç°æœ‰çˆ¬è™«æŠ“å–ç¬”è®°
    - å°†å›¾ç‰‡ä¸æ–‡å­—ä¿å­˜åˆ° backend/downloads/æ ‡é¢˜/ ä¸‹
    """
    print(f"\nğŸ“¥ [ä¸‹è½½] å¼€å§‹çˆ¬å–å¹¶ä¿å­˜: {request.url}")

    scraper = XHSScraper()
    try:
        data = await scraper.scrape_note(request.url)
        if not data:
            raise HTTPException(status_code=400, detail="æŠ“å–å¤±è´¥")
    finally:
        await scraper.close()

    # å¦‚æœå‰ç«¯ä¼ äº†è‡ªå®šä¹‰ base_dirï¼Œåˆ™è¦†ç›–é»˜è®¤ DOWNLOAD_ROOT
    global DOWNLOAD_ROOT
    original_root = DOWNLOAD_ROOT
    try:
        if request.base_dir:
            # æ”¯æŒç»å¯¹è·¯å¾„ & ç›¸å¯¹ backend çš„è·¯å¾„
            if os.path.isabs(request.base_dir):
                DOWNLOAD_ROOT = request.base_dir
            else:
                DOWNLOAD_ROOT = os.path.join(BASE_DIR, request.base_dir)
            os.makedirs(DOWNLOAD_ROOT, exist_ok=True)

        saved = await _save_note_to_disk(data)
    finally:
        # è¿˜åŸå…¨å±€é…ç½®ï¼Œé¿å…å½±å“å…¶ä»–è¯·æ±‚
        DOWNLOAD_ROOT = original_root

    print(f"âœ… [ä¸‹è½½] å·²ä¿å­˜åˆ°æ–‡ä»¶å¤¹: {saved['folder']}")

    return saved

# === æ–°å¢ï¼šæ‰¹é‡è§£ææ¥å£ ===
@app.post("/api/batch_parse", response_model=BatchParseResponse)
async def batch_parse(request: BatchParseRequest):
    """
    æ‰¹é‡è§£æå°çº¢ä¹¦ç¬”è®°é“¾æ¥
    """
    print(f"\nğŸ“¥ [æ‰¹é‡è§£æ] å¼€å§‹è§£æ {len(request.urls)} ä¸ªé“¾æ¥...")
    
    notes: List[ParsedNote] = []
    failed: List[Dict[str, str]] = []
    
    # å¹¶å‘è§£æï¼ˆé™åˆ¶å¹¶å‘æ•°é¿å…è¿‡è½½ï¼‰
    semaphore = asyncio.Semaphore(3)  # æœ€å¤š3ä¸ªå¹¶å‘
    
    async def parse_single(url: str):
        async with semaphore:
            scraper = XHSScraper()
            try:
                data = await scraper.scrape_note(url)
                if not data:
                    failed.append({"url": url, "error": "æŠ“å–å¤±è´¥"})
                    return
                
                note_id = _generate_note_id(url)
                cover_image = data['images'][0] if data.get('images') else None
                
                notes.append(ParsedNote(
                    id=note_id,
                    url=url,
                    title=data.get('title', ''),
                    content=data.get('content', ''),
                    tags=data.get('tags', []),
                    images=data.get('images', []),
                    coverImage=cover_image
                ))
                print(f"âœ… [æ‰¹é‡è§£æ] æˆåŠŸ: {data.get('title', '')[:30]}")
            except Exception as e:
                print(f"âŒ [æ‰¹é‡è§£æ] å¤±è´¥ {url}: {e}")
                failed.append({"url": url, "error": str(e)})
            finally:
                await scraper.close()
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰è§£æä»»åŠ¡
    await asyncio.gather(*[parse_single(url) for url in request.urls])
    
    print(f"âœ… [æ‰¹é‡è§£æ] å®Œæˆ: æˆåŠŸ {len(notes)} ä¸ªï¼Œå¤±è´¥ {len(failed)} ä¸ª")
    return BatchParseResponse(notes=notes, failed=failed)


# === æ–°å¢ï¼šZIPä¸‹è½½æ¥å£ï¼ˆæ¨èï¼Œç›´æ¥ä¸‹è½½åˆ°ç”¨æˆ·æœ¬åœ°ï¼‰ ===
@app.post("/api/download_zip")
async def download_zip(request: ZipDownloadRequest):
    """
    å°†ç¬”è®°æ‰“åŒ…æˆZIPå¹¶è¿”å›ç»™å‰ç«¯ä¸‹è½½
    """
    print(f"\nğŸ“¦ [ZIPä¸‹è½½] å¼€å§‹æ‰“åŒ…: {request.note_data.get('title', '')}")
    
    try:
        title = request.note_data.get('title', 'xhs_note')
        content = request.note_data.get('content', '')
        tags = request.note_data.get('tags', [])
        origin_url = request.note_data.get('origin_url', '')
        images = request.note_data.get('images', [])
        
        # ç¡®å®šè¦ä¸‹è½½çš„å›¾ç‰‡
        images_to_download = images
        if request.selected_image_indices is not None:
            images_to_download = [images[i] for i in request.selected_image_indices if 0 <= i < len(images)]
        
        # åˆ›å»ºå†…å­˜ä¸­çš„ZIPæ–‡ä»¶
        zip_buffer = io.BytesIO()
        folder_name = _sanitize_filename(title)
        
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            # 1. æ·»åŠ æ–‡æœ¬æ–‡ä»¶
            text_filename = f"{folder_name}.txt"
            text_content = f"{title}\n\n{content}\n\n"
            if tags:
                text_content += f"æ ‡ç­¾: {', '.join(tags)}\n"
            if origin_url:
                text_content += f"æ¥æºé“¾æ¥: {origin_url}\n"
            
            zip_file.writestr(text_filename, text_content.encode('utf-8'))
            
            # 2. ä¸‹è½½å¹¶æ·»åŠ å›¾ç‰‡
            for idx, img_url in enumerate(images_to_download, start=1):
                img_data = await download_image_as_bytes(img_url)
                if not img_data:
                    continue
                
                mime = img_data.get("mime_type", "image/jpeg").lower()
                ext = "jpg"
                if "png" in mime:
                    ext = "png"
                elif "webp" in mime:
                    ext = "webp"
                elif "gif" in mime:
                    ext = "gif"
                
                img_filename = f"image_{idx}.{ext}"
                zip_file.writestr(img_filename, img_data["data"])
                print(f"   - å·²æ·»åŠ å›¾ç‰‡: {img_filename}")
        
        zip_buffer.seek(0)
        zip_filename = f"{folder_name}.zip"
        
        print(f"âœ… [ZIPä¸‹è½½] æ‰“åŒ…å®Œæˆ: {zip_filename} ({len(zip_buffer.getvalue())} bytes)")
        
        # è¿”å›ZIPæ–‡ä»¶
        return Response(
            content=zip_buffer.getvalue(),
            media_type="application/zip",
            headers={
                "Content-Disposition": f'attachment; filename="{zip_filename}"',
                "Content-Length": str(len(zip_buffer.getvalue()))
            }
        )
        
    except Exception as e:
        print(f"âŒ [ZIPä¸‹è½½] å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰“åŒ…ZIPå¤±è´¥: {str(e)}")


# === æ—§ç‰ˆï¼šé€‰æ‹©æ€§ä¸‹è½½æ¥å£ï¼ˆä¿å­˜åˆ°æœåŠ¡å™¨ï¼Œä¿ç•™ç”¨äºå…¼å®¹ï¼‰ ===
@app.post("/api/selective_download", response_model=SelectiveDownloadResponse)
async def selective_download(request: SelectiveDownloadRequest):
    """
    é€‰æ‹©æ€§ä¸‹è½½ç¬”è®°ï¼ˆæ”¯æŒé€‰æ‹©ç‰¹å®šå›¾ç‰‡ï¼‰- ä¿å­˜åˆ°æœåŠ¡å™¨
    æ³¨æ„ï¼šFly.io æ–‡ä»¶ç³»ç»Ÿæ˜¯ä¸´æ—¶çš„ï¼Œå»ºè®®ä½¿ç”¨ /api/download_zip æ¥å£
    """
    print(f"\nğŸ“¥ [é€‰æ‹©æ€§ä¸‹è½½] å¼€å§‹ä¸‹è½½: {request.note_data.get('title', '')}")
    
    # å¦‚æœå‰ç«¯ä¼ äº†è‡ªå®šä¹‰ base_dirï¼Œåˆ™è¦†ç›–é»˜è®¤ DOWNLOAD_ROOT
    global DOWNLOAD_ROOT
    original_root = DOWNLOAD_ROOT
    try:
        if request.base_dir:
            if os.path.isabs(request.base_dir):
                DOWNLOAD_ROOT = request.base_dir
            else:
                DOWNLOAD_ROOT = os.path.join(BASE_DIR, request.base_dir)
            os.makedirs(DOWNLOAD_ROOT, exist_ok=True)
        
        saved = await _save_note_to_disk(
            request.note_data, 
            selected_indices=request.selected_image_indices
        )
    finally:
        DOWNLOAD_ROOT = original_root
    
    print(f"âœ… [é€‰æ‹©æ€§ä¸‹è½½] å·²ä¿å­˜åˆ°æ–‡ä»¶å¤¹: {saved['folder']}")
    return SelectiveDownloadResponse(**saved)


# === æ–°å¢ï¼šæµè§ˆæ–‡ä»¶å¤¹æ¥å£ ===
@app.post("/api/browse_folder", response_model=BrowseFolderResponse)
async def browse_folder(request: BrowseFolderRequest):
    """
    æµè§ˆæ–‡ä»¶å¤¹ï¼ˆç”¨äºå‰ç«¯é€‰æ‹©ä¿å­˜è·¯å¾„ï¼‰
    """
    try:
        if request.path:
            target_path = request.path
            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            if not os.path.isabs(target_path):
                target_path = os.path.join(BASE_DIR, target_path)
        else:
            # é»˜è®¤è¿”å›backendç›®å½•
            target_path = BASE_DIR
        
        if not os.path.exists(target_path):
            raise HTTPException(status_code=404, detail="è·¯å¾„ä¸å­˜åœ¨")
        
        if not os.path.isdir(target_path):
            raise HTTPException(status_code=400, detail="ä¸æ˜¯æœ‰æ•ˆçš„æ–‡ä»¶å¤¹è·¯å¾„")
        
        # è·å–çˆ¶ç›®å½•
        parent_path = None
        if target_path != BASE_DIR and os.path.dirname(target_path) != target_path:
            parent_path = os.path.dirname(target_path)
        
        # åˆ—å‡ºæ–‡ä»¶å¤¹å†…å®¹
        items: List[FolderItem] = []
        try:
            for item_name in sorted(os.listdir(target_path)):
                item_path = os.path.join(target_path, item_name)
                if os.path.isdir(item_path):
                    items.append(FolderItem(
                        name=item_name,
                        path=item_path,
                        is_directory=True
                    ))
        except PermissionError:
            raise HTTPException(status_code=403, detail="æ— æƒé™è®¿é—®è¯¥æ–‡ä»¶å¤¹")
        
        return BrowseFolderResponse(
            current_path=target_path,
            items=items,
            parent_path=parent_path
        )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æµè§ˆæ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    port = int(os.getenv("PORT", "8000"))
    uvicorn.run(app, host="0.0.0.0", port=port)